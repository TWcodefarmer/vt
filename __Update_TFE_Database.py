# === æ¨™æº–åº« Standard Library ===
import os
import glob
import datetime
import time
import re
import zipfile
import logging

# === ç¬¬ä¸‰æ–¹å¥—ä»¶ Third-party Packages ===
import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
from bs4 import BeautifulSoup
import requests

# === æ—¥èªŒ logging è¨­å®š Logging Settings ===
logging.basicConfig(
    filename='__Update_TFE_Database.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def data_cleansing_TFX_futures_OriginalTickFiles(filename, contract):
    '''
    input filename: åŸå§‹æœŸäº¤æ‰€æœŸè²¨RPT or CSVæª”
    contract: é æœŸè™•ç†åˆç´„åç¨±
    output: æ¸…æ´—å¾Œdf
    '''
    # è®€å–CSVæª”æ¡ˆä¸¦æŒ‡å®šç·¨ç¢¼ç‚ºBig5
    df = pd.read_csv(filename, encoding='Big5',low_memory=False)
    # æŒ‡å®šè‹±æ–‡æ¬„å
    eng_columnames = ['date', 'contract', 'duedmonth', 'timestamp', 'price', 'volume', '1', '2', '3']
    # æ›¿æ›è³‡æ–™æ¡†çš„æ¬„åç‚ºè‹±æ–‡æ¬„å
    df.columns = eng_columnames
    # ç§»é™¤contractå’Œduedmonthå­—ä¸²æ¬„ä½çš„å‰å¾Œç©ºç™½å­—å…ƒ
    df['contract'] = df['contract'].str.strip().astype('string').astype('category')
    df['duedmonth'] = df['duedmonth'].str.strip().astype('string').astype('category')
    # é¸æ“‡contractç‚º"TX"
    df = df.query('contract == @contract').iloc[:, 0:6]
    # å°‡dateå’Œtimestampæ¬„ä½çš„è³‡æ–™å‹åˆ¥è½‰æ›ç‚ºå­—ä¸²
    df['date'] = df['date'].astype('int64').astype('string')
    df['timestamp'] = df['timestamp'].astype('int64').astype('string').str.zfill(6)
    # æ–°å¢datetimeæ¬„ä½ï¼Œåˆä½µdateå’Œtimestampæ¬„ä½
    df['datetime'] = df['date'] + df['timestamp']
    # å°‡datetimeæ¬„ä½è½‰æ›ç‚ºæ—¥æœŸæ™‚é–“å‹åˆ¥
    df['datetime'] = pd.to_datetime(df['datetime'], format='%Y%m%d%H%M%S')
    # æ ¹æ“šdatetimeæ¬„ä½é€²è¡Œæ’åº
    df = df.sort_values('datetime')
    # è¨ˆç®—capæ¬„ä½ï¼Œæˆäº¤é‡ç‚ºè²·è³£åˆä½µ
    df['cap'] = df['price'] * df['volume']
    # æ•´ç†æ¬²è¼¸å‡ºæ ¼å¼
    df.drop(['date', 'timestamp'], axis=1,inplace=True)
    df.insert(0, 'datetime', df.pop('datetime'))
    # é‡è¨­index åŠ é€Ÿè™•ç†é€Ÿåº¦
    df.reset_index(drop=True, inplace=True)
    return df

def data_cleansing_TFX_options_OriginalTickFiles(filename, contract):
    '''
    input filename: åŸå§‹æœŸäº¤æ‰€é¸æ“‡æ¬ŠRPT or CSVæª”
    contract: é æœŸè™•ç†åˆç´„åç¨±
    output: æ¸…æ´—å¾Œdf
    '''
    # è®€å–CSVæª”æ¡ˆä¸¦æŒ‡å®šç·¨ç¢¼ç‚ºBig5
    df = pd.read_csv(filename, encoding='Big5',low_memory=False)
    # æŒ‡å®šè‹±æ–‡æ¬„å
    eng_columnames = ['date', 'contract', 'strike', 'duedmonth', 'callput', 'timestamp', 'price', 'volume', '1']
    # æ›¿æ›è³‡æ–™æ¡†çš„æ¬„åç‚ºè‹±æ–‡æ¬„å
    df.columns = eng_columnames
    # ç§»é™¤contractå’Œduedmonthå­—ä¸²æ¬„ä½çš„å‰å¾Œç©ºç™½å­—å…ƒ
    df['contract'] = df['contract'].str.strip().astype('string').astype('category')
    df['duedmonth'] = df['duedmonth'].str.strip().astype('string').astype('category')
    df['callput'] = df['callput'].str.strip().astype('string').astype('category')
    # # é¸æ“‡contractç‚º"TX"ä¸”duedmonthç‚º"202307"çš„è³‡æ–™ä¸¦é¸å–å…¨éƒ¨æ¬„ä½
    df = df.query('contract == @contract').iloc[:, 0:8]
    # # å°‡dateå’Œtimestampæ¬„ä½çš„è³‡æ–™å‹åˆ¥è½‰æ›ç‚ºå­—ä¸²
    df['date'] = df['date'].astype('int64').astype('string')
    df['timestamp'] = df['timestamp'].astype('int64').astype('string').str.zfill(6)
    df['strike'] = df['strike'].astype('int64').astype('string').astype('category')
    # æ–°å¢datetimeæ¬„ä½ï¼Œåˆä½µdateå’Œtimestampæ¬„ä½
    df['datetime'] = df['date'] + df['timestamp']
    # å°‡datetimeæ¬„ä½è½‰æ›ç‚ºæ—¥æœŸæ™‚é–“å‹åˆ¥
    df['datetime'] = pd.to_datetime(df['datetime'], format='%Y%m%d%H%M%S')
    # # æ ¹æ“šdatetimeæ¬„ä½é€²è¡Œæ’åº
    df = df.sort_values('datetime')
    # # è¨ˆç®—capæ¬„ä½ï¼Œæˆäº¤é‡ç‚ºè²·è³£åˆä½µ
    df['cap'] = df['price'] * df['volume']
    # # æ•´ç†æ¬²è¼¸å‡ºæ ¼å¼
    df.drop(['date', 'timestamp'], axis=1,inplace=True)
    df.insert(0, 'datetime', df.pop('datetime'))
    # é‡è¨­index åŠ é€Ÿè™•ç†é€Ÿåº¦
    df.reset_index(drop=True, inplace=True)
    return df

def future_crawler(CSVorRPT, wait_time, target_folder='TFE_tick_rpt_fut'):
    # ç¢ºä¿ç›®æ¨™è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(target_folder, exist_ok=True)

    # åˆ¤æ–·è¦ä¸‹è¼‰çš„æª”æ¡ˆé¡å‹
    filetype = 'CSV' if CSVorRPT.lower() == 'csv' else ''

    # æœŸäº¤æ‰€è¿‘30æ—¥ç¶²å€
    target = 'https://www.taifex.com.tw/cht/3/dlFutPrevious30DaysSalesData'
    response = requests.get(target)
    sp = BeautifulSoup(response.text, 'html.parser')
    targetsouplist = sp.find_all('input', class_='btn_orange')

    # æ‰¾åˆ°æ‰€æœ‰å¯ä¸‹è¼‰æª”æ¡ˆçš„æ—¥æœŸ
    filedates = []
    for i in targetsouplist:
        if 'https://www.taifex.com.tw/file/taifex/Dailydownload/Dailydownload/Daily' in str(i):
            y_m_d = (str(i).split('_')[2:4] + str(i).split('_')[4:5][0].split('.'))[0:3]
            ymd = '_'.join(y_m_d)
            if ymd not in filedates:
                filedates.append(ymd)

    # æ‰¾åˆ°æ‰€æœ‰ç™¼å¸ƒæ™‚é–“
    realeasetimelist = []
    targetsouplist = sp.find_all('td', align='center')
    pattern = r'\d{4}/\d{2}/\d{2} (AM|PM) \d{2}:\d{2}:\d{2}'
    for i in targetsouplist:
        match = re.search(pattern, str(i))
        if match:
            i = str(i).split('>')[1].split('<')[0]
            realeasetimelist.append(i)

    df1 = None
    if len(filedates) == len(realeasetimelist):
        df1 = pd.DataFrame({'ç™¼å¸ƒæ™‚é–“': realeasetimelist, 'æª”æ¡ˆæ—¥æœŸ': filedates})

    for i, d in enumerate(filedates):
        realeasedate = datetime.datetime.strptime(realeasetimelist[i], '%Y/%m/%d %p %I:%M:%S').strftime('%Y_%m_%d')
        releasetime = datetime.datetime.strptime(realeasetimelist[i], '%Y/%m/%d %p %I:%M:%S').time()

        if d == realeasedate and releasetime >= datetime.datetime.now().replace(hour=16, minute=0, second=0, microsecond=0).time():
            downloadlink = f'https://www.taifex.com.tw/file/taifex/Dailydownload/Dailydownload{filetype}/Daily_{d}.zip'
            filename = downloadlink.split('/')[-1]
            file_path = os.path.join(target_folder, filename)
            rpt_path = os.path.join(target_folder, filename.replace('.zip', '.rpt'))

            if os.path.exists(file_path) or os.path.exists(rpt_path):
                print('æ­¤æœŸè²¨å·²å­˜åœ¨ï¼Œè·³éä¸ä¸‹è¼‰:', filename)
            else:
                with urllib.request.urlopen(downloadlink) as response:
                    zipcontent = response.read()
                    with open(file_path, 'wb') as f:
                        f.write(zipcontent)
                    print(f'ä¸‹è¼‰æˆåŠŸ: {filename}')
                    for j in range(wait_time):
                        print(f'ä¼‘æ¯{j + 1}ç§’')
                        time.sleep(1)
        else:
            print(f'é€™å¤© {d} æª”æ¡ˆç›®å‰åªæœ‰åŠå¤©å¤œç›¤ï¼Œè·³é')
    print('ä¸‹è¼‰ä»»å‹™åŸ·è¡Œå®Œç•¢')
    return df1

def option_crawler(CSVorRPT, wait_time, target_folder='TFE_tick_rpt_opt'):
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(target_folder, exist_ok=True)

    # æ±ºå®šæª”æ¡ˆé¡å‹
    filetype = 'CSV' if CSVorRPT.lower() == 'csv' else ''

    # é¸æ“‡æ¬Šè¿‘30æ—¥è³‡æ–™ç¶²å€
    target = 'https://www.taifex.com.tw/cht/3/dlOptPrevious30DaysSalesData'
    response = requests.get(target)
    sp = BeautifulSoup(response.text, 'html.parser')
    targetsouplist = sp.find_all('input', class_='btn_orange')

    # æŠ“æ—¥æœŸ
    filedates = []
    for i in targetsouplist:
        if 'https://www.taifex.com.tw/file/taifex/Dailydownload/OptionsDailydownload/OptionsDaily' in str(i):
            y_m_d = (str(i).split('_')[2:4] + str(i).split('_')[4:5][0].split('.'))[0:3]
            ymd = '_'.join(y_m_d)
            if ymd not in filedates:
                filedates.append(ymd)

    # æŠ“ç™¼å¸ƒæ™‚é–“
    realeasetimelist = []
    targetsouplist = sp.find_all('td', align='center')
    pattern = r'\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}'
    for i in targetsouplist:
        match = re.search(pattern, str(i))
        if match:
            i = str(i).split('>')[1].split('<')[0]
            realeasetimelist.append(i)

    df2 = None
    if len(filedates) == len(realeasetimelist):
        df2 = pd.DataFrame({'ç™¼å¸ƒæ™‚é–“': realeasetimelist, 'æª”æ¡ˆæ—¥æœŸ': filedates})

    # é–‹å§‹ä¸‹è¼‰
    for i, d in enumerate(filedates):
        realeasedate = datetime.datetime.strptime(realeasetimelist[i], '%Y/%m/%d %H:%M:%S').strftime('%Y_%m_%d')
        releasetime = datetime.datetime.strptime(realeasetimelist[i], '%Y/%m/%d %H:%M:%S').time()

        if d == realeasedate and releasetime >= datetime.datetime.now().replace(hour=16, minute=0, second=0, microsecond=0).time():
            downloadlink = f'https://www.taifex.com.tw/file/taifex/Dailydownload/OptionsDailydownload{filetype}/OptionsDaily_{d}.zip'
            filename = downloadlink.split('/')[-1]
            file_path = os.path.join(target_folder, filename)
            rpt_path = os.path.join(target_folder, filename.replace('.zip', '.rpt'))

            if os.path.exists(file_path) or os.path.exists(rpt_path):
                print('æ­¤é¸æ“‡æ¬Šæª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éä¸ä¸‹è¼‰:', filename)
            else:
                with urllib.request.urlopen(downloadlink) as response:
                    zipcontent = response.read()
                    with open(file_path, 'wb') as f:
                        f.write(zipcontent)
                    print(f'ä¸‹è¼‰æˆåŠŸ: {filename}')
                    for j in range(wait_time):
                        print(f'ä¼‘æ¯{j + 1}ç§’')
                        time.sleep(1)
        else:
            print(f'é€™å¤© {d} æª”æ¡ˆåªæœ‰åŠå¤©å¤œç›¤ï¼Œè·³é')
    print('é¸æ“‡æ¬Šè³‡æ–™ä¸‹è¼‰å®Œç•¢')
    return df2

def extract_zip_files(target_folder):
    # ç¢ºä¿ç›®æ¨™è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(target_folder, exist_ok=True)

    existing_rpts = set()
    for filename in os.listdir(target_folder):
        if filename.endswith('.rpt'):
            existing_rpts.add(filename)

    for filename in os.listdir(target_folder):
        if filename.endswith('.zip') and filename.replace('.zip', '.rpt') not in existing_rpts:
            with zipfile.ZipFile(os.path.join(target_folder, filename), 'r') as zip_ref:
                zip_ref.extractall(target_folder)
            print(f'{filename} å·²æˆåŠŸè§£å£“ç¸®ã€‚')

def get_tick_data_from_TFE():
    """
    å¾æœŸäº¤æ‰€ (TFE) ä¸‹è¼‰ä¸¦è§£å£“æœŸè²¨ã€é¸æ“‡æ¬Šé€ç­†è³‡æ–™ (tick data)ã€‚
    Download and extract futures & options tick data from TFE.

    æ­¥é©Ÿï¼š
    1. ä¸‹è¼‰æœŸè²¨è³‡æ–™ä¸¦è§£å£“ã€‚
    2. ä¸‹è¼‰é¸æ“‡æ¬Šè³‡æ–™ä¸¦è§£å£“ã€‚
    """
    # è¨­å®šæœŸè²¨ tick è³‡æ–™å„²å­˜çš„è³‡æ–™å¤¾åç¨±
    foldername = 'TFE_tick_rpt_fut'
    # ä¸‹è¼‰æœŸè²¨ tick è³‡æ–™ï¼Œç­‰å¾…æ™‚é–“è¨­ç‚º 3 ç§’
    future_crawler('rpt', wait_time=1, target_folder=foldername)
    # è§£å£“ä¸‹è¼‰çš„ zip æª”æ¡ˆè‡³æŒ‡å®šè³‡æ–™å¤¾
    extract_zip_files(target_folder=foldername)

    # è¨­å®šé¸æ“‡æ¬Š tick è³‡æ–™å„²å­˜çš„è³‡æ–™å¤¾åç¨±
    foldername = 'TFE_tick_rpt_opt'
    # ä¸‹è¼‰é¸æ“‡æ¬Š tick è³‡æ–™ï¼Œç­‰å¾…æ™‚é–“è¨­ç‚º 3 ç§’
    option_crawler('rpt', wait_time=1, target_folder=foldername)
    # è§£å£“ä¸‹è¼‰çš„ zip æª”æ¡ˆè‡³æŒ‡å®šè³‡æ–™å¤¾
    extract_zip_files(target_folder=foldername)

def combine_rpts_to_single_csv(folder_name):
    """
    å°‡æŒ‡å®šè³‡æ–™å¤¾å…§æ‰€æœ‰ .rpt æª”æ¡ˆåˆä½µæˆä¸€å€‹æœªæ¸…æ´—çš„å¤§ CSV æª”æ¡ˆï¼Œä¸¦å„²å­˜æ–¼ç•¶å‰å·¥ä½œç›®éŒ„ã€‚

    åƒæ•¸:
        folder_name (str): åŒ…å« .rpt æª”æ¡ˆçš„è³‡æ–™å¤¾åç¨±ã€‚

    å›å‚³:
        str: åˆä½µå¾Œå„²å­˜çš„ CSV æª”æ¡ˆåç¨±ï¼ˆå«æ™‚é–“æˆ³è¨˜ï¼‰ã€‚
    """
    # å»ºç«‹è³‡æ–™å¤¾çš„å®Œæ•´è·¯å¾‘
    folder_path = os.path.join(os.getcwd(), folder_name)

    # å°‹æ‰¾è©²è³‡æ–™å¤¾å…§æ‰€æœ‰ .rpt æª”æ¡ˆï¼ˆä»¥ Big5 ç·¨ç¢¼å„²å­˜çš„ CSV æ ¼å¼ï¼‰
    rpt_files = glob.glob(os.path.join(folder_path, '*.rpt'))

    # å°‡æ‰€æœ‰ .rpt æª”æ¡ˆè®€å…¥ç‚º DataFrame ä¸¦å­˜å…¥æ¸…å–®
    dataframes = [pd.read_csv(file, encoding='Big5', low_memory=False) for file in rpt_files]

    # å°‡æ‰€æœ‰è³‡æ–™åˆä½µç‚ºä¸€å€‹å¤§ DataFrame
    combined_df = pd.concat(dataframes, ignore_index=True)

    # ç”¢ç”Ÿå„²å­˜ç”¨æª”æ¡ˆåç¨±ï¼ˆå¸¶æœ‰æ™‚é–“æˆ³è¨˜ï¼‰
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f'{folder_name}_uncleaned_{timestamp}.csv'

    # å°‡åˆä½µå¾Œè³‡æ–™å„²å­˜ç‚º Big5 ç·¨ç¢¼çš„ CSV æª”æ¡ˆ
    combined_df.to_csv(output_filename, index=False, encoding='Big5')

    return output_filename

def update_futures_feather_db(
    feather_db='near_fut_cleaned.feather', 
    rpt_folder='TFE_tick_rpt_fut',
    contract_code='TX'
):
    """è‡ªå‹•æ•´åˆä¸¦æ›´æ–°å°æŒ‡æœŸ feather è³‡æ–™åº«
    
    åƒæ•¸:
        feather_db (str): feather æª”æ¡ˆåç¨±
        rpt_folder (str): rpt æª”æ¡ˆæ‰€åœ¨è³‡æ–™å¤¾
        contract_code (str): åˆç´„ä»£ç¢¼ï¼ˆä¾‹å¦‚ TXï¼‰
    """
    try:
        if os.path.exists(feather_db):
            df_0 = pd.read_feather(feather_db)
            time_first = df_0.iloc[0, 0]   # æœ€èˆŠçš„æ—¥æœŸ first date
            time_last = df_0.iloc[-1, 0]   # æœ€æ–°çš„æ—¥æœŸ last date
            print('æª”æ¡ˆåç¨±', feather_db, 'èµ·å§‹æ—¥æœŸ:', time_first, 'æœ€æ–°æ—¥æœŸ:', time_last)
            folder_path = os.path.join(os.getcwd(), rpt_folder)
            rpt_files = glob.glob(os.path.join(folder_path, '*.rpt'))
            for file_path in rpt_files:
                # æ“·å– rpt æª”åä¸­çš„æ—¥æœŸ
                try:
                    time_1 = datetime.datetime.strptime(
                        (''.join(file_path.split('Daily_')[1:2]).split('.')[0]), '%Y_%m_%d')
                except Exception as e:
                    print(f"æª”åè§£æå¤±æ•—: {file_path}, éŒ¯èª¤: {e}")
                    continue
                if time_last < time_1:
                    print('æ–°å¢è³‡æ–™åº«æ—¥æœŸ:', time_1)
                    newdf = data_cleansing_TFX_futures_OriginalTickFiles(file_path, contract_code)
                    df_0 = pd.concat([df_0, newdf], ignore_index=True)
                    df_0['contract'] = df_0['contract'].astype('category')
                    df_0['duedmonth'] = df_0['duedmonth'].astype('category')
            df_0.to_feather(feather_db)
        else:
            print(f"æª”æ¡ˆ '{feather_db}' ä¸å­˜åœ¨ï¼Œè£½é€ æ–°è³‡æ–™åº«")
            # å‡è¨­ combine_rpts_to_single_csv è¼¸å‡ºæ•´åˆéçš„æª”å
            filename = combine_rpts_to_single_csv(rpt_folder)
            # æ¸…æ´—åŸå§‹å¤§CSV ä¸¦å­˜æˆfeather
            data_cleansing_TFX_futures_OriginalTickFiles(filename, contract_code).to_feather(feather_db)
        
        # æª¢æŸ¥æ—¥æœŸæ˜¯å¦éå¢ (is_monotonic_increasing è‹±æ–‡å–®å­—: å–®èª¿éå¢)
        check_date_increasing = pd.read_feather(feather_db)['datetime'].is_monotonic_increasing
        logging.info(f' : {check_date_increasing} : å°æŒ‡æ—¥æœŸæ˜¯å¦æ­£ç¢ºéå¢')
        return check_date_increasing
    except Exception as ex:
        logging.error(f"è‡ªå‹•æ•´åˆæœŸè²¨è³‡æ–™åº«ç™¼ç”Ÿç•°å¸¸: {ex}")
        return False

def update_options_feather_db(
    feather_db='near_opt_cleaned.feather',
    rpt_folder='TFE_tick_rpt_opt',
    contract_code='TXO'
):
    """
    è‡ªå‹•æ•´åˆä¸¦æ›´æ–°å°æŒ‡é¸æ“‡æ¬Š feather è³‡æ–™åº«

    åƒæ•¸:
        feather_db (str): feather æª”æ¡ˆåç¨±
        rpt_folder (str): rpt æª”æ¡ˆè³‡æ–™å¤¾åç¨±
        contract_code (str): åˆç´„ä»£ç¢¼ï¼ˆå¦‚ 'TXO'ï¼‰
    å›å‚³:
        bool: æ—¥æœŸæ˜¯å¦å–®èª¿éå¢ï¼ˆis_monotonic_increasing è‹±æ–‡å–®å­—ï¼šå–®èª¿éå¢ï¼‰
    """
    try:
        if os.path.exists(feather_db):
            df_0 = pd.read_feather(feather_db)
            time_first = df_0.iloc[0, 0]   # æœ€èˆŠçš„æ—¥æœŸ first date
            time_last = df_0.iloc[-1, 0]   # æœ€æ–°çš„æ—¥æœŸ last date
            print('æª”æ¡ˆåç¨±', feather_db, 'èµ·å§‹æ—¥æœŸ:', time_first, 'æœ€æ–°æ—¥æœŸ:', time_last)
            folder_path = os.path.join(os.getcwd(), rpt_folder)
            rpt_files = glob.glob(os.path.join(folder_path, '*.rpt'))
            for file_path in rpt_files:
                try:
                    time_1 = datetime.datetime.strptime(
                        (''.join(file_path.split('Daily_')[1:2]).split('.')[0]), '%Y_%m_%d')
                except Exception as e:
                    print(f"æª”åè§£æå¤±æ•—: {file_path}, éŒ¯èª¤: {e}")
                    continue
                if time_last < time_1:
                    print('æ–°å¢è³‡æ–™åº«æ—¥æœŸ:', time_1)
                    newdf = data_cleansing_TFX_options_OriginalTickFiles(file_path, contract_code)
                    df_0 = pd.concat([df_0, newdf], ignore_index=True)
                    # å„æ¬„ä½å‹åˆ¥è½‰æ›
                    df_0['contract'] = df_0['contract'].astype('category')
                    df_0['duedmonth'] = df_0['duedmonth'].astype('category')
                    df_0['strike'] = df_0['strike'].astype('category')
                    df_0['callput'] = df_0['callput'].astype('category')
            df_0.to_feather(feather_db)
        else:
            print(f"æª”æ¡ˆ '{feather_db}' ä¸å­˜åœ¨ï¼Œè£½é€ æ–°è³‡æ–™åº«")
            filename = combine_rpts_to_single_csv(rpt_folder)
            data_cleansing_TFX_options_OriginalTickFiles(filename, contract_code).to_feather(feather_db)

        # æª¢æŸ¥æ—¥æœŸæ˜¯å¦å–®èª¿éå¢ï¼ˆis_monotonic_increasingï¼‰
        check_date_increasing = pd.read_feather(feather_db)['datetime'].is_monotonic_increasing
        logging.info(f' : {check_date_increasing} : å°é¸æ—¥æœŸæ˜¯å¦æ­£ç¢ºéå¢')
        return check_date_increasing
    except Exception as ex:
        logging.error(f"è‡ªå‹•æ•´åˆå°æŒ‡é¸æ“‡æ¬Šè³‡æ–™åº«ç™¼ç”Ÿç•°å¸¸: {ex}")
        return False

def process_futures_minute_data(
    feather_path='near_fut_cleaned.feather',
    output_csv='RESULT_contract_end_times.csv',
    output_feather='RESULT_fut.feather',
    start_year=2011,
    end_year=None,
    pre_min=30
):
    """
    æœŸè²¨è³‡æ–™åˆ†Kèˆ‡çµç®—æ—¥è£½ä½œ (Futures minute resampling & contract settlement day)

    åƒæ•¸èªªæ˜:
        feather_path: è¼¸å…¥ feather æª”æ¡ˆè·¯å¾‘
        output_csv: è¼¸å‡ºæ¯æœˆçµç®—æ—¥ CSV
        output_feather: è¼¸å‡ºé€£çºŒåˆç´„ feather
        start_year: è™•ç†èµ·å§‹å¹´
        end_year: è™•ç†çµæŸå¹´ (é è¨­ä»Šå¹´)
        pre_min: æå‰å¤šå°‘åˆ†é˜è¦–ç‚ºçµæŸæ™‚é–“ (é è¨­30)
    """

    # é è¨­åˆ°ä»Šå¹´
    if end_year is None:
        end_year = datetime.datetime.now().year

    df = pd.read_feather(feather_path)
    df.set_index('datetime', inplace=True)

    # åˆ†é˜ç´šåˆ¥èšåˆï¼ˆresampleï¼‰
    resampled_df = df.groupby(['duedmonth'], observed=True).resample(
        'min', label='right', closed='right'
    ).agg({'volume': 'sum', 'cap': 'sum'}).reset_index()

    # é‡æ–°è¨ˆç®—åƒ¹æ ¼æ¬„ä½
    resampled_df['price'] = resampled_df['cap'] / resampled_df['volume']

    # åˆå§‹åŒ– (initialize) è®Šæ•¸
    target_df = pd.DataFrame()
    latest_time = None
    end_times = []

    for i in range(start_year, end_year+2):
        for j in range(1, 13):
            duedmonth = str(i) + str(j).zfill(2)
            current_df = resampled_df.query('duedmonth == @duedmonth')
            if current_df.empty:
                continue
            # çµæŸæ™‚é–“é» (æå‰Nåˆ†é˜)
            datetime_ = current_df.iloc[-1].datetime - pd.Timedelta(minutes=pre_min)
            datetime_plus_1 = datetime_ + pd.Timedelta(minutes=1)
            matched_rows = current_df[current_df['datetime'] == datetime_]
            if matched_rows.empty:
                continue
            index_start = current_df.iloc[0].name
            index_end = matched_rows.index[0]
            end_price = matched_rows.price.iloc[0]
            end_times.append({'duedmonth': duedmonth, 'end_time': datetime_})
            new_target = current_df.loc[index_start:index_end]
            if latest_time:
                new_target = new_target[new_target['datetime'] >= latest_time]
            latest_time = datetime_plus_1
            target_df = pd.concat([target_df, new_target], ignore_index=True)

    # çµç®—æ—¥è™•ç†
    end_times_df = pd.DataFrame(end_times)
    end_times_df['time_only'] = pd.to_datetime(end_times_df['end_time']).dt.time
    # åƒ…ä¿ç•™ 13:00:00 çµç®—çš„åˆç´„
    end_times_df = end_times_df[end_times_df['time_only'] == datetime.time(13, 0, 0)]
    end_times_df = end_times_df.drop(columns=['time_only'])
    # åªä¿ç•™ç•¶æœˆä»¥å‰çš„è³‡æ–™
    current_ym = int(datetime.datetime.now().strftime("%Y%m"))
    end_times_df = end_times_df[end_times_df['duedmonth'].astype(int) <= current_ym]

    # è¼¸å‡º
    end_times_df.to_csv(output_csv, index=False)
    print(f"âœ… æ¯æœˆåˆç´„çµç®—æ—¥å·²å„²å­˜ {output_csv}")
    print('æœ€è¿‘ä¸€æ¬¡åˆç´„çµç®—æœˆä»½:', end_times_df.tail(1).duedmonth.values[0])

    target_df = target_df.dropna(subset=['price']).reset_index(drop=True)
    target_df.to_feather(output_feather)
    print(f"âœ… å°æŒ‡åˆ†Kå·²æ¸…ç†ä¸”åˆä½µç‚ºé€£çºŒåˆç´„ {output_feather}")

    return end_times_df, target_df

def calculate_contract_price_gap(
    feather_path='near_fut_cleaned.feather',
    contract_end_csv='RESULT_contract_end_times.csv',
    output_csv='RESULT_contract_price_gap.csv',
    max_attempts=20
):
    """
    è¨ˆç®—åˆç´„åƒ¹å·®ï¼ˆspread/gapï¼‰ä¸¦å„²å­˜ç‚º CSV

    åƒæ•¸èªªæ˜:
        feather_path: è¿‘æœˆæœŸè²¨è³‡æ–™ feather è·¯å¾‘
        contract_end_csv: åˆç´„çµæŸæ—¥ csv è·¯å¾‘
        output_csv: çµæœå„²å­˜è·¯å¾‘
        max_attempts: æ¯å€‹çµæŸæ—¥æœ€å¤šå¾€å‰å¹¾åˆ†é˜æ‰¾ï¼ˆé è¨­ 20ï¼‰
    """
    near_fut_df = pd.read_feather(feather_path)
    # éæ¿¾æ‰ 'duedmonth' å« '/' çš„è³‡æ–™ï¼ˆæ ¼å¼ç•°å¸¸ï¼‰
    near_fut_df = near_fut_df[~near_fut_df['duedmonth'].astype(str).str.contains('/')]
    contract_dates_df = pd.read_csv(contract_end_csv)
    results = []

    for idx, row in contract_dates_df.iterrows():
        temp_datetime = row.iloc[1]  # çµç®—æ™‚é–“ï¼ˆdatetimeï¼‰
        found_data = False

        for _ in range(max_attempts):
            temp_df = near_fut_df[near_fut_df['datetime'] == temp_datetime]
            if not temp_df.empty:
                temp_df = temp_df.sort_values('duedmonth')
                contracts = temp_df.duedmonth.unique()
                if len(contracts) > 1:
                    contract_now = contracts[0]
                    contract_next = contracts[1]
                    # é©—è­‰ç¬¬ä¸€å€‹åˆç´„å¿…é ˆç‚ºç•¶æœˆ
                    current_month = pd.to_datetime(temp_datetime).strftime('%Y%m')
                    if contract_now != current_month:
                        print(f"âŒ ERROR: åˆç´„ {contract_now} ä¸ç¬¦åˆæ—¥æœŸ {temp_datetime} çš„ç•¶æœˆ {current_month}")
                        temp_datetime = pd.to_datetime(temp_datetime) - pd.Timedelta(minutes=1)
                        continue
                    contract_now_price = (
                        temp_df.query('duedmonth == @contract_now').cap.sum() /
                        temp_df.query('duedmonth == @contract_now').volume.sum()
                    )
                    contract_next_price = (
                        temp_df.query('duedmonth == @contract_next').cap.sum() /
                        temp_df.query('duedmonth == @contract_next').volume.sum()
                    )
                    gap = contract_next_price - contract_now_price
                    results.append({
                        'datetime': temp_datetime,
                        'contract_now': contract_now,
                        'contract_next': contract_next,
                        'contract_now_price': contract_now_price,
                        'contract_next_price': contract_next_price,
                        'gap': gap
                    })
                    found_data = True
                    break
            # æ™‚é–“å¾€å‰æ¨ç§»ä¸€åˆ†é˜
            temp_datetime = pd.to_datetime(temp_datetime) - pd.Timedelta(minutes=1)
        if not found_data:
            print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ° {row.iloc[1]} é™„è¿‘çš„äº¤æ˜“è³‡æ–™ï¼Œè·³éè©²æ™‚é–“é»ã€‚")

    # è½‰æˆ DataFrame ä¸¦ç´¯åŠ  gap
    results_df = pd.DataFrame(results)
    results_df['gap_cumsum'] = results_df.gap.cumsum()
    results_df.to_csv(output_csv, index=False)
    print(f"âœ… è³‡æ–™åº«çš„åˆç´„åƒ¹å·®å·²å„²å­˜: '{output_csv}'")
    # åˆç´„åç¨±æª¢æŸ¥
    contracts = sorted(results_df["contract_now"].tolist())
    print("åˆç´„åç¨±:", contracts)

    # å¯é¸: åˆç´„é€£çºŒæ€§æª¢æŸ¥
    # is_continuous = all((int(contracts[i + 1]) - int(contracts[i])) in {1, 11} for i in range(len(contracts) - 1))
    # if is_continuous:
    #     print("âœ…ç„¡é ˆç†æœƒä¸Šè¿° ERROR åˆç´„é€£çºŒç„¡ç¼ºæ¼ ")
    # else:
    #     print("ğŸ” åˆç´„é€£çºŒæ€§æª¢æŸ¥: âŒ åˆç´„ä¸é€£çºŒï¼Œå­˜åœ¨ç¼ºæ¼ æª¢æŸ¥ä¸Šè¿° ERROR")
    return results_df

def adjust_continuous_contract_gap(
    fut_feather='RESULT_fut.feather',
    price_gap_csv='RESULT_contract_price_gap.csv',
    output_feather='RESULT_gap_fixed.feather'
):
    """
    è™•ç†é€£çºŒåˆç´„æ›æœˆåƒ¹å·®èª¿æ•´ (adjust continuous contract price gap)

    åƒæ•¸èªªæ˜:
        fut_feather: è®€å–çš„é€£çºŒåˆç´„featheræª”æ¡ˆ
        price_gap_csv: åˆç´„åƒ¹å·®çµæœcsv
        output_feather: èª¿æ•´å¾Œå„²å­˜çš„featheræª”æ¡ˆ
    """

    # è®€å–é€£çºŒåˆç´„åˆ†Kè³‡æ–™
    target_df = pd.read_feather(fut_feather)
    # è®€å–åƒ¹å·®è³‡æ–™
    df = pd.read_csv(price_gap_csv)
    # æº–å‚™gapå­—å…¸
    gap_df = df[['contract_next', 'gap_cumsum']].copy()
    gap_df['contract_next'] = gap_df['contract_next'].astype(str)
    gap_dict = dict(zip(gap_df['contract_next'], gap_df['gap_cumsum']))
    # å‹æ…‹çµ±ä¸€
    target_df['duedmonth'] = target_df['duedmonth'].astype(str)
    # èª¿æ•´åƒ¹æ ¼ adjusted_price
    target_df['price'] = target_df.apply(
        lambda row: row['price'] - gap_dict.get(row['duedmonth'], 0),
        axis=1
    )
    # capé‡æ–°è¨ˆç®—
    target_df['cap'] = (target_df['volume'] * target_df['price']) // 1
    # å„²å­˜
    target_df.to_feather(output_feather)
    print(f"âœ… èª¿æ•´å¾Œçš„åƒ¹æ ¼å·²å„²å­˜è‡³ '{output_feather}'")
    return target_df

def plot_price_by_duedmonth(df, start, end): # æœˆåˆç´„å‹æ…‹ç¢ºèª å¯è§€å¯Ÿåˆç´„æ˜¯å¦èšåˆéŒ¯èª¤
    """
    ç¹ªè£½æŒ‡å®šå€é–“å…§çš„åƒ¹æ ¼åœ–ï¼Œæ ¹æ“š duedmonth åˆ†çµ„ï¼Œä¸¦è¨­ç½®é»‘è‰²èƒŒæ™¯ã€‚

    åƒæ•¸:
    df : DataFrame
        åŒ…å« 'duedmonth' å’Œ 'price' æ¬„ä½çš„è³‡æ–™é›†ã€‚
    start : int
        èµ·å§‹ç´¢å¼•ã€‚
    end : int
        çµæŸç´¢å¼•ã€‚
    """
    # ç¯©é¸æŒ‡å®šå€é–“çš„è³‡æ–™
    filtered_df = df.iloc[start:end]

    # è¨­ç½®é»‘åº•æ¨£å¼
    plt.style.use('dark_background')

    # ä½¿ç”¨ç´¢å¼•ä½œç‚º x è»¸ä¾†ç¹ªè£½åœ–å½¢
    plt.figure(figsize=(10, 6))
    for duedmonth, group in filtered_df.groupby(['duedmonth'], observed=True):
        plt.plot(group.index, group['price'], label=f"duedmonth: {duedmonth}")

    plt.xlabel('Index', color='white')
    plt.ylabel('Price', color='white')
    plt.title(f'Price vs Index by Duedmonth (Rows {start} to {end})', color='white')
    plt.legend(facecolor='black', edgecolor='white')
    plt.tight_layout()
    plt.show()


get_tick_data_from_TFE()

check = update_futures_feather_db()
print("å°æŒ‡æ—¥æœŸæ˜¯å¦æ­£ç¢ºéå¢:", check)

check = update_options_feather_db()
print("å°é¸æ—¥æœŸæ˜¯å¦æ­£ç¢ºéå¢:", check)

end_times_df, target_df = process_futures_minute_data(pre_min=30) # å¯èª¿æå‰åˆ†é˜æ•¸
calculate_contract_price_gap(max_attempts=20) # å¯è‡ªè¨‚æœå°‹åˆ†é˜æ•¸
adjusted_df = adjust_continuous_contract_gap()

# debug å·¥å…· è§€çœ‹åˆç´„åƒ¹æ ¼é€£çºŒæ€§
# plot_price_by_duedmonth(target_df,0,-1)
# plot_price_by_duedmonth(adjusted_df,0,-1)

def recycle_temp_files():
    # Remove .rpt files in ./TFE_tick_rpt_fut
    for rpt_file in glob.glob('./TFE_tick_rpt_fut/*.rpt'):
        try:
            os.remove(rpt_file)
            print(f"\033[1;32m[OK]\033[0m Deleted: {rpt_file}")
        except Exception as e:
            print(f"\033[1;31m[ERROR]\033[0m Failed to delete: {rpt_file}, reason: {e}")

    # Remove .rpt files in ./TFE_tick_rpt_opt
    for rpt_file in glob.glob('./TFE_tick_rpt_opt/*.rpt'):
        try:
            os.remove(rpt_file)
            print(f"\033[1;32m[OK]\033[0m Deleted: {rpt_file}")
        except Exception as e:
            print(f"\033[1;31m[ERROR]\033[0m Failed to delete: {rpt_file}, reason: {e}")

    # Remove TFE_tick_rpt_fut_uncleaned_*.csv in the main directory
    for csv_file in glob.glob('./*TFE_tick_rpt_fut_uncleaned_*.csv'):
        try:
            os.remove(csv_file)
            print(f"\033[1;32m[OK]\033[0m Deleted: {csv_file}")
        except Exception as e:
            print(f"\033[1;31m[ERROR]\033[0m Failed to delete: {csv_file}, reason: {e}")

    # Remove TFE_tick_rpt_opt_uncleaned_*.csv in the main directory
    for csv_file in glob.glob('./*TFE_tick_rpt_opt_uncleaned_*.csv'):
        try:
            os.remove(csv_file)
            print(f"\033[1;32m[OK]\033[0m Deleted: {csv_file}")
        except Exception as e:
            print(f"\033[1;31m[ERROR]\033[0m Failed to delete: {csv_file}, reason: {e}")

# ç”¨æ³•
recycle_temp_files()
